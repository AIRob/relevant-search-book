{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import json\n",
      "import os\n",
      "import pickle\n",
      "    \n",
      "elasticSearchUrl = \"http://es:9200\"\n",
      "\n",
      "tmdb_api_key = os.environ[\"TMDB_API_KEY\"]\n",
      "\n",
      "import requests\n",
      "http = requests.Session()\n",
      "http.params={'api_key': tmdb_api_key}\n",
      "\n",
      "es = requests.Session();\n",
      "\n",
      "# POSSIBLE SEARCHES\n",
      "# most popular (don't know what popular means though): https://api.themoviedb.org/3/discover/movie?api_key=ae700a17fe68acb1deb66c34b41c174f&sort_by=popularity.desc\n",
      "# movies sorted by vote (instead of popularity) with a vote count above 100 https://api.themoviedb.org/3/discover/movie?api_key=ae700a17fe68acb1deb66c34b41c174f&sort_by=vote_average.desc&vote_count.gte=100\n",
      "# \"best\" movies released in the past 5 years: https://api.themoviedb.org/3/discover/movie?api_key=ae700a17fe68acb1deb66c34b41c174f&sort_by=vote_average.desc&vote_count.gte=100&primary_release_year.gte=2009\n",
      "# * most voted upon (good way to get movies that everyone will know but which will include good and bad movies) https://api.themoviedb.org/3/discover/movie?api_key=ae700a17fe68acb1deb66c34b41c174f&sort_by=vote_count.desc&vote_count.gte=100\n",
      "# most popular kids movies https://api.themoviedb.org/3/discover/movie?api_key=ae700a17fe68acb1deb66c34b41c174f&sort_by=vote_count.desc&certification_country=US&certification.lte=G\n",
      "# highest revenues https://api.themoviedb.org/3/discover/movie?api_key=ae700a17fe68acb1deb66c34b41c174f&sort_by=revenue.desc\n",
      "\n",
      "# movie example ids [24428, 27205, 19995, 155, 68721, 70160, 68718, 49051, 49026, 37724]\n",
      "# person id 3223\n",
      "\n",
      "# Other searches\n",
      "# All people associated with a movie curl \"https://api.themoviedb.org/3/movie/24428/credits?api_key=ae700a17fe68acb1deb66c34b41c174f\" | jq '.[\"cast\"][][\"name\"]'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Elasticsearch!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For our search engine, we'll be using Elasticsearch. Elasticsearch is a modern Lucene-based search engine with an emphasis on ease of use. In the previous chapter, you were introduced to the basic processes, data structures, and components of a Lucene-based search engine. Though considered extremely mature, Lucene is it's still a fairly advanced Java library. Working directly with Lucene (1) requires you to code in Java (2) tends to require a great deal of verbosity and (3) trips up beginners with many of the low-level concerns. For this reason, Elasticsearch works hard to wrap Lucene's features in a RESTful interface that focusses on providing sane conventions for search applications.\n",
      "\n",
      "It's important to note this book is not about Elasticsearch. This book is about developing a discipline for relevant search. Elasticsearch is simply our example search engine for exploring relevance. For this reason, we do not attempt to be comprehensive. Further, we'll move through the search engine in vertical slices -- starting right away with a fully-functioning search implementation. If you're curious to explore more about any component in Elasticsearch, there are many great books written. The project itself provides great documentation at http://elasticsearch.org. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TMDB"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this chapter, we'll be using \"The Movie Database\" (TMDB) as our data set. TMDB is a popular online movie and tv-show database. We're excited about this data set as it contains several components many search applications need to deal with. These include\n",
      "\n",
      "1. Longer text features -- the text within synopsis, and user reviews\n",
      "2. Shorter text features -- director and actor names, titles, etc\n",
      "3. Numerical Quality Features -- user ratings, movie revenue, awards like Oscar \n",
      "4. Other features often important to search -- movie release dates, etc\n",
      "\n",
      "We'll be using TMDB through it's API. Usage of the API requires a key which can be acquired at the TMDB website. If you'd like to follow long with the examples, please obtain a key.\n",
      "\n",
      "## Organization of the TMDB API\n",
      "\n",
      "The TMDB API is organized around a series of endpoints that list information about movies, such as '/movies/popular' (list of movies by popularity) or '/movies/top_rated/' (list of movies by top rated). This is really just a starting point, after we snag a series of movies we want to search, we'll grab deeper information from each movie from endpoints such as `/movie/{id}/reviews` to get each review. Further, many of these endpoints contain lists that return 20 items a piece. We may need to pass a page parameter to page through the response."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our examples will be composed in Python. Why Python? Python is a highly-readable imperative language that looks and feels like psuedocode. You don't need to know Python to follow along (just pretend we're writing psuedocode). We're not doing anything fancy with the language, so these examples should still be easy to follow along. Another thing to note is we'll omit a fair amount of boilerplate error checking. This is important stuff! But our focus is on teaching the nitty-gritty of search relevance -- not exploring a robust programming solution. We also don't rely on any of the excellent Elasticsearch clients out there in order to keep things simple. If you're building a real Elasticsearch application, be sure to explore the great client libraries out there!\n",
      "\n",
      "You'll be able to explore this code at our github repository in more detail."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Enough Talk, Let's Search!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Basic ETL and Indexing\n",
      "\n",
      "A great deal of search relevance revolves around text features. To get started, we're going to index a few pieces of text about popular movies into Elasticsearch. We'll do this by accessing the TMDB movies endpoint, issuing a GET request and indexing the title, overview, and title. In this first example, we'll try to be a bit more verbose about what we're doing, commenting carefully as we move forward. \n",
      "\n",
      "Like we said before, first we'll snag the ids of some popular movies:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movieIds = [];\n",
      "numMoviesToGrab = 200\n",
      "numPages = numMoviesToGrab / 20\n",
      "\n",
      "for page in range(1, numPages + 1):\n",
      "    httpResp = http.get('https://api.themoviedb.org/3/movie/top_rated', params={'page': page})  #(1)\n",
      "    jsonResponse = json.loads(httpResp.text) #(2)\n",
      "    movies = jsonResponse['results']\n",
      "    for movie in movies:\n",
      "        movieIds.append(movie['id'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the listing above, at line 1 we issue a GET request to the TMDB API. In the next line, we parse the HTTP text body, loading the JSON body into a Python dictionary. The entry \"results\" contains the meat of the response, holding each movie for us to work with. The TMDB Json response looks something like:\n",
      "\n",
      "```\n",
      "{\n",
      " \"total_results\": 2769, \n",
      " \"total_pages\": 139, \n",
      " \"page\": 1, \n",
      " \"results\": [\n",
      "  {\n",
      "   \"poster_path\": \"/m0UDkSPoVkmNfXFR9FN13yewy4B.jpg\", \n",
      "   \"title\": \"Interstellar\", \n",
      "   \"release_date\": \"2014-11-05\", \n",
      "   \"popularity\": 18.1103049211704, \n",
      "   \"original_title\": \"Interstellar\", \n",
      "   \"backdrop_path\": \"/xu9zaAevzQ5nnrsXN6JcahLnG4i.jpg\", \n",
      "   \"vote_count\": 1431, \n",
      "   \"video\": false, \n",
      "   \"adult\": false, \n",
      "   \"vote_average\": 8.7, \n",
      "   \"id\": 157336\n",
      "  }, \n",
      "  {\n",
      "   \"poster_path\": \"/lIv1QinFqz4dlp5U4lQ6HaiskOZ.jpg\", \n",
      "   \"title\": \"Whiplash\", \n",
      "    ...\n",
      "  }, \n",
      "  ...\n",
      "```\n",
      "\n",
      "The subsequent loop simply builds a list of ids. We'll use this list of ids to pull back even deeper information about movies, by accessing each movie endpoint individually."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movieDict = {}\n",
      "for movieId in movieIds:\n",
      "    httpResp = http.get(\"https://api.themoviedb.org/3/movie/%s\" % movieId)\n",
      "    movie = json.loads(httpResp.text)\n",
      "    genreList = []\n",
      "    for genreObj in movie['genres']:\n",
      "        genreList.append(genreObj['name'])\n",
      "    movie['genres'] = genreList;\n",
      "    movieDict[movieId] = movie"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you might imagine, the movie endpoint returns an even more detailed version of the JSON object in the movie list above. We now have a bunch of additional information on each movie, contained within the movieDict dictionary. This includes details such as the movie's title, an overview, and a tagline. One bit of dirty business to simplify our search operation is the genre field. TMDB gives us an object that looks like `{\"id\": 1234, \"name\": \"drama\"}`. We take this complex object, and do a tiny amount of transformation pulling out the name and omitting the other information.\n",
      "\n",
      "Now with some interesting fields, we will index these documents into Elasticsearch using the bulk index API. Elasticsearch has several ways to index documents. We'll predominantly use the bulk index API that allows us to efficiently index multiple documents at the same time. The important thing to note about the bulk API is the format is a bit pecuilar, it takes the form\n",
      "\n",
      "    {JSON COMMAND} <newline>\n",
      "    {new document} <newline>\n",
      "    ...\n",
      "\n",
      "So below, we build up multiple JSON documents, each on its own line to prepare the documents for the bulk API:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Indexing %i movies\" % len(movieDict.keys())\n",
      "bulkMovies = \"\"\n",
      "for id, movie in movieDict.iteritems():\n",
      "    addCmd = {\"index\": {\"_index\": \"tmdb\", \"_type\": \"movie\", \"_id\": movie[\"id\"]}}\n",
      "    esDoc  = {\"title\": movie['title'], \"genres\": movie['genres'],\n",
      "               'overview': movie['overview'], 'tagline': movie['tagline']}\n",
      "    bulkMovies += json.dumps(addCmd) + \"\\n\" + json.dumps(esDoc) + \"\\n\"\n",
      "requests.post(elasticSearchUrl + \"/_bulk\", data=bulkMovies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Indexing 200 movies\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "<Response [200]>"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Congratulations! We've built our first ETL pipeline. Here we've \n",
      "\n",
      "1. extracted information from an external system, \n",
      "2. transformed the data into a form amenible to the search engine, and\n",
      "3. indexed the data into Elasticsearch\n",
      "\n",
      "Further, in the commands above by telling Elasticsearch about a new index (`_index: tmdb`) and about a new type (`_type: movie`). We've created both an index (synonomous with a database) and a type of documents (synonomous with a SQL table). In the future, when we want to search or otherwise interact with the movies we've indexed, we'll reference `tmdb/movie/` in the path of the Elasticsearch URL.\n",
      "\n",
      "## Basic Search User Experience\n",
      "\n",
      "Now we can search!\n",
      "\n",
      "For our movie application, we've got to figure out how to respond to user searches from our search bar. To do this, we'll be issuing queries to Elasticsearch using the Query DSL. Elasticsearch's Query DSL describes how to execute a search using a JSON format that specfies to Elasticsearch exactly how to search results. Here you specify factors such as required clauses, clauses that should not be included, boosts, field weight, and other factors. These factors influence matching (boolean search) as well as scoring (ranked retrieval). As we'll see the art of building a powerful search experience is creating features for each document in the search engine with some thought as to how those features will be married to a search strategy.\n",
      "\n",
      "Being a fairly new relevance engineer, we'll just start by taking the user's query and executing a search over all the available fields:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search = {\n",
      "    'query': {\n",
      "        'match': { \n",
      "            '_all': 'space exploration movie'  #User's query\n",
      "        }\n",
      "    }\n",
      "}\n",
      "httpResp = requests.get(elasticSearchUrl + '/tmdb/movie/_search', data=json.dumps(search))\n",
      "searchHits = json.loads(httpResp.text)['hits']\n",
      "print \"Relevance Score\\t\\tMovie Title\\t\\tOverview\"\n",
      "for hit in searchHits['hits']:\n",
      "    print \"%s\\t\\t%s\\t\\t%s\" % (hit['_score'], hit['_source']['title'], hit['_source']['overview'][:50])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Relevance Score\t\tMovie Title\t\tOverview\n",
        "0.09059772\t\tThe Lego Movie\t\tAn ordinary Lego mini-figure, mistakenly thought t\n",
        "0.08549146\t\tThe Apartment\t\tBud Baxter is a minor clerk in a huge New York ins\n",
        "0.082213044\t\tSolaris\t\tGround control has been receiving strange transmis\n",
        "0.08007783\t\tGravity\t\tDr. Ryan Stone (Sandra Bullock), a brilliant medic\n",
        "0.07966096\t\tThe Man from Earth\t\tAn impromptu goodbye party for Professor John Oldm\n",
        "0.07966096\t\tInterstellar\t\tInterstellar chronicles the adventures of a group \n",
        "0.07046832\t\tA Separation\t\tThe movie is centered on a couple, Nader and Simin\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at these results, they're not terrible. But the first two movies are way off the mark! The movies Solaris, Gravity, on seem to be much more relevant. What happened? Why did the two movies that appear to have nothing to do with Space Exploration shoot up to the top of the search  results?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Solving our First Relevance Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start to unravel the magic. First thing we'll do is to try to understand the query we ran from the last section. Why was it the case that we got results like \"The Lego Movie\" and \"The Apartment\" for the search \"space exploration movie\"? How can we diagnose teh problem? How can we go about solving the problem?\n",
      "\n",
      "## Explaining The Results\n",
      "\n",
      "First we need to work to diagnose exactly why the results came back in the form they did. To do this, we'll utilize Elasticsearch's explain API. With the explain API, we'll see the components that actually make up the relevance score. A lot of books gloss over this topic as advanced, but its pretty foundational for your work as a relevance engineer. So we'll use the opportunity of diagnosing this search to dig into this explain. What we'll see is the result of a similarity calculation between the features extracted from the text when indexing and similar features extracted from the search query.\n",
      "\n",
      "There are two things we need to understand\n",
      "\n",
      "1. How the Elasticsearch query DSL was translated into a query of the underlying inverted index data structures\n",
      "2. What were the components of the underlying scoring\n",
      "\n",
      "### Explaining The Query\n",
      "\n",
      "The first thing we'll do is ask Elasticsearch to decompose our search query into something that describes the underlying Lucene query. This gives us deeper insight into how the query itself works against the Lucene data structures. To do this, we'll use the query explain endpoint to get an understanding of what's happening in Lucene query syntax:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search = {\n",
      "    'query': {\n",
      "        'match': { \n",
      "            '_all': 'space exploration movie'  #User's query\n",
      "        }\n",
      "    }\n",
      "}\n",
      "httpResp = requests.get(elasticSearchUrl + '/tmdb/movie/_validate/query?explain', data=json.dumps(search))\n",
      "json.loads(httpResp.text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "{u'_shards': {u'failed': 0, u'successful': 1, u'total': 1},\n",
        " u'explanations': [{u'explanation': u'filtered(_all:space _all:exploration _all:movie)->cache(_type:movie)',\n",
        "   u'index': u'tmdb',\n",
        "   u'valid': True}],\n",
        " u'valid': True}"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Lucene Query Syntax\n",
      "\n",
      "Ignoring all the hints related to filtering and caching (a topic for another book) let's examine the juicy nugget here, the Lucene query: `_all:space _all:exploration _all:movie`. Lucene query syntax is a low-level, precise way of specifying a search. The syntax reflects search operators power search users may be familiar with. Lucene query syntax describes the requirements of a relevant document a bit more closely tied to how Lucene itself will perform the search. A Lucene query specifies very precise match/score behavior against flat Lucene documents. Understanding the basics of Lucene queries helps us debug our search relevance.\n",
      "\n",
      "Lucene queries are composed of a number of clauses, each one specifying an important match in the underlying document. Each clause takes the form `[+/-]<fieldName>:<query>`. The most important part of the clause is the component that specifies the match itself: `<fieldName>:<query>`. Recall that despite any syntactic sugar might layer on top, Lucene documents are in-fact flat. In the clauses above, we are querying the Lucene `_all` field. What are we asking the `_all` field? In our case, we're simply querying for three specific terms: \"space\", \"exploration\", and \"movie\". Each clause here is a simple **term query**. We'll encounter additional queries througout this book such as queries that extend past one term to phrases.\n",
      "\n",
      "If you recall the boolean search discussion from the last chapter, Lucene is able combine queries with boolean operators such as OR, AND, and NOT. In reality, Lucene applies boolean operators to individual clauses. You see this with part of  the first component of the clause `+/-` reflects the nature of the clause. A `+` indicates the clause is mandatory. Do not return a document unless this query matches. A `-` indicates just the opposite, that this clause must not match in the document.\n",
      "\n",
      "Omitting this operator, simply indicates that something SHOULD match -- telling the search engine that more of these matches are a good thing -- score documents that match higher. Because SHOULD tends to give the search engine hints on what factors it should score higher, the various ways we manipulate relevance tend to get expressed as SHOULD queries. \n",
      "\n",
      "We'll encounter more advanced forms of Lucene query syntax as we explore deeper into the search engine's capabilities.\n",
      "\n",
      "#### Ok, so what does our Query Do?\n",
      "\n",
      "Now that we know something about Lucene query syntax, we can decompose what exactly our query does. We asked the Query DSL to execute an Elasticsearch match query against the `_all` field. Elasticsearch took our query:\n",
      "\n",
      "> space exploration movie\n",
      "\n",
      "and appears to have decomposed it using analysis into three terms to search for\n",
      "\n",
      "> [space] [exploration] [movie]\n",
      "\n",
      "further, Elasticsearch issued three SHOULD queries, telling Lucene that movies should be scored more highly as more of these clauses are satisfied:\n",
      "\n",
      "> _all:space _all:exploration _all:movie\n",
      "\n",
      "If we envision the inverted index as we do in Chapter 2, we can see how this might work. Let's look at each term that we're querying in the all field.\n",
      "\n",
      "<pre>\n",
      "> field _all\n",
      ">  term space\n",
      ">    doc 0\n",
      ">      <metadata about space in this doc>\n",
      ">    doc 2\n",
      ">      ...\n",
      ">  ... \n",
      ">  term exploration\n",
      ">    doc 3\n",
      ">     ...\n",
      ">  term movie\n",
      ">    doc 6\n",
      ">     ...\n",
      ">  ...\n",
      "</pre>\n",
      "\n",
      "Recalling what we learned in Chapter 2, we can envision exactly how this query works to collect results. First we visit `space` in the term dictionary, and collect all the documents listed. As we mentioned in chapter 2, we end up with a bit mask of matching documents. Next step? We visit `exploration` in the term dictionary, picking up even more documents. We finish by also including the matches to `space` in the search index.\n",
      "\n",
      "We end up with a list of matching document identifiers that looks like `[0, 2, 3, 6, ... ].\n",
      "\n",
      "This is of course boolean search -- a big OR query. But, of course, a search result must do much more than just boolean searches. Modern search engines must give users the right answer -- often on the first result! Simply returning an unranked list of results won't satisfy our users. This is precisely what the <metadata> block above will help us with -- ranked retrieval based on statistics about this term."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### And why do these non-space movies match our space query?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why does \"The Lego Movie\" and \"The Apartment\" come up so highly when Lucene is asked to search with `_all:space _all:exploration _all:movie`? What we'll need to do is ask Elasticsearch to explain itself. How did it arrive at the scoring numbers it did? What factors created the score?\n",
      "\n",
      "There are a couple of ways to do this, but as we often want to see the explain information with the search results, its often convenient to set `explain: true` when issuing the search query itself. This will return an `_explanation` in the search results. We hinted at above how each document a term appears in contains a bit of metadata. We'll see how this metadata (spefically the term frequency) allows us to give each matching document a relevance score. We further see how the various clauses we described above combine to provide an overall relevance score that factors in the impact of each match.\n",
      "\n",
      "\n",
      "Let's reissue a search to see what is going on and reflect on whether we're really searching like we expect:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search = {\n",
      "    'explain': True, # enable the explain/debug info\n",
      "    'query': {\n",
      "        'match': { \n",
      "            '_all': 'space exploration movie'  #User's query\n",
      "        }\n",
      "    }\n",
      "}\n",
      "httpResp = requests.get(elasticSearchUrl + '/tmdb/movie/_search', data=json.dumps(search))\n",
      "jsonResp = json.loads(httpResp.text)\n",
      "print \"Explain for %s\" % jsonResp['hits']['hits'][0]['_source']['title']\n",
      "print json.dumps(jsonResp['hits']['hits'][0]['_explanation'], indent=True)\n",
      "print \"Explain for %s\" % jsonResp['hits']['hits'][1]['_source']['title']\n",
      "print json.dumps(jsonResp['hits']['hits'][1]['_explanation'], indent=True)\n",
      "print \"Explain for %s\" % jsonResp['hits']['hits'][3]['_source']['title']\n",
      "print json.dumps(jsonResp['hits']['hits'][3]['_explanation'], indent=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Explain for The Lego Movie\n",
        "{\n",
        " \"description\": \"product of:\", \n",
        " \"value\": 0.09059772, \n",
        " \"details\": [\n",
        "  {\n",
        "   \"description\": \"sum of:\", \n",
        "   \"value\": 0.27179316, \n",
        "   \"details\": [\n",
        "    {\n",
        "     \"description\": \"weight(_all:movie in 27) [PerFieldSimilarity], result of:\", \n",
        "     \"value\": 0.27179316, \n",
        "     \"details\": [\n",
        "      {\n",
        "       \"description\": \"score(doc=27,freq=1.0 = termFreq=1.0\\n), product of:\", \n",
        "       \"value\": 0.27179316, \n",
        "       \"details\": [\n",
        "        {\n",
        "         \"description\": \"queryWeight, product of:\", \n",
        "         \"value\": 0.5441669, \n",
        "         \"details\": [\n",
        "          {\n",
        "           \"description\": \"idf(docFreq=1, maxDocs=40)\", \n",
        "           \"value\": 3.9957323\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"queryNorm\", \n",
        "           \"value\": 0.13618703\n",
        "          }\n",
        "         ]\n",
        "        }, \n",
        "        {\n",
        "         \"description\": \"fieldWeight in 27, product of:\", \n",
        "         \"value\": 0.49946654, \n",
        "         \"details\": [\n",
        "          {\n",
        "           \"description\": \"tf(freq=1.0), with freq of:\", \n",
        "           \"value\": 1.0, \n",
        "           \"details\": [\n",
        "            {\n",
        "             \"description\": \"termFreq=1.0\", \n",
        "             \"value\": 1.0\n",
        "            }\n",
        "           ]\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"idf(docFreq=1, maxDocs=40)\", \n",
        "           \"value\": 3.9957323\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"fieldNorm(doc=27)\", \n",
        "           \"value\": 0.125\n",
        "          }\n",
        "         ]\n",
        "        }\n",
        "       ]\n",
        "      }\n",
        "     ]\n",
        "    }\n",
        "   ]\n",
        "  }, \n",
        "  {\n",
        "   \"description\": \"coord(1/3)\", \n",
        "   \"value\": 0.33333334\n",
        "  }\n",
        " ]\n",
        "}\n",
        "Explain for The Apartment\n",
        "{\n",
        " \"description\": \"product of:\", \n",
        " \"value\": 0.08549145, \n",
        " \"details\": [\n",
        "  {\n",
        "   \"description\": \"sum of:\", \n",
        "   \"value\": 0.25647435, \n",
        "   \"details\": [\n",
        "    {\n",
        "     \"description\": \"weight(_all:movie in 20) [PerFieldSimilarity], result of:\", \n",
        "     \"value\": 0.25647435, \n",
        "     \"details\": [\n",
        "      {\n",
        "       \"description\": \"score(doc=20,freq=1.0 = termFreq=1.0\\n), product of:\", \n",
        "       \"value\": 0.25647435, \n",
        "       \"details\": [\n",
        "        {\n",
        "         \"description\": \"queryWeight, product of:\", \n",
        "         \"value\": 0.5612442, \n",
        "         \"details\": [\n",
        "          {\n",
        "           \"description\": \"idf(docFreq=1, maxDocs=48)\", \n",
        "           \"value\": 4.178054\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"queryNorm\", \n",
        "           \"value\": 0.1343315\n",
        "          }\n",
        "         ]\n",
        "        }, \n",
        "        {\n",
        "         \"description\": \"fieldWeight in 20, product of:\", \n",
        "         \"value\": 0.45697463, \n",
        "         \"details\": [\n",
        "          {\n",
        "           \"description\": \"tf(freq=1.0), with freq of:\", \n",
        "           \"value\": 1.0, \n",
        "           \"details\": [\n",
        "            {\n",
        "             \"description\": \"termFreq=1.0\", \n",
        "             \"value\": 1.0\n",
        "            }\n",
        "           ]\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"idf(docFreq=1, maxDocs=48)\", \n",
        "           \"value\": 4.178054\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"fieldNorm(doc=20)\", \n",
        "           \"value\": 0.109375\n",
        "          }\n",
        "         ]\n",
        "        }\n",
        "       ]\n",
        "      }\n",
        "     ]\n",
        "    }\n",
        "   ]\n",
        "  }, \n",
        "  {\n",
        "   \"description\": \"coord(1/3)\", \n",
        "   \"value\": 0.33333334\n",
        "  }\n",
        " ]\n",
        "}\n",
        "Explain for Gravity\n",
        "{\n",
        " \"description\": \"product of:\", \n",
        " \"value\": 0.08007783, \n",
        " \"details\": [\n",
        "  {\n",
        "   \"description\": \"sum of:\", \n",
        "   \"value\": 0.24023348, \n",
        "   \"details\": [\n",
        "    {\n",
        "     \"description\": \"weight(_all:space in 31) [PerFieldSimilarity], result of:\", \n",
        "     \"value\": 0.24023348, \n",
        "     \"details\": [\n",
        "      {\n",
        "       \"description\": \"score(doc=31,freq=2.0 = termFreq=2.0\\n), product of:\", \n",
        "       \"value\": 0.24023348, \n",
        "       \"details\": [\n",
        "        {\n",
        "         \"description\": \"queryWeight, product of:\", \n",
        "         \"value\": 0.5441669, \n",
        "         \"details\": [\n",
        "          {\n",
        "           \"description\": \"idf(docFreq=1, maxDocs=40)\", \n",
        "           \"value\": 3.9957323\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"queryNorm\", \n",
        "           \"value\": 0.13618703\n",
        "          }\n",
        "         ]\n",
        "        }, \n",
        "        {\n",
        "         \"description\": \"fieldWeight in 31, product of:\", \n",
        "         \"value\": 0.4414702, \n",
        "         \"details\": [\n",
        "          {\n",
        "           \"description\": \"tf(freq=2.0), with freq of:\", \n",
        "           \"value\": 1.4142135, \n",
        "           \"details\": [\n",
        "            {\n",
        "             \"description\": \"termFreq=2.0\", \n",
        "             \"value\": 2.0\n",
        "            }\n",
        "           ]\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"idf(docFreq=1, maxDocs=40)\", \n",
        "           \"value\": 3.9957323\n",
        "          }, \n",
        "          {\n",
        "           \"description\": \"fieldNorm(doc=31)\", \n",
        "           \"value\": 0.078125\n",
        "          }\n",
        "         ]\n",
        "        }\n",
        "       ]\n",
        "      }\n",
        "     ]\n",
        "    }\n",
        "   ]\n",
        "  }, \n",
        "  {\n",
        "   \"description\": \"coord(1/3)\", \n",
        "   \"value\": 0.33333334\n",
        "  }\n",
        " ]\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oh boy, that's some terrifying looking gobbly gook. What's happening is a JSON decomposition of arithmetic. Each number on the outside is explain by the contents of that JSON object's details list. The \"details\" list in turn contains even more details about the calculation, each with its own explanation. At the outermost explain we have the score attached to the document. As we move deeper into the `details` we see how that score is calculated in increasing complexity. \n",
      "\n",
      "Eventually, we get to layers that look like, `weight(_all:space)`. At this layer, it appears we're starting to describe something about the scoring of a specific match. Indeed here, we can begin to see something interesting. Matches for `weight(_all:movie)` in \"Lego Movie\" come out with a higher score than `weight(_all:space)` in \"Gravity\"! This is quite surprising. Why is this? If we go deeper here, we can explore the scoring components derrived from data in the underlying index. We'll discuss this in a second.\n",
      "\n",
      "Before we dive into those details, lets try to figure out what's wrapping these individual matches. We can view the individual match queries that directly consult the inverted index as individual lego blocks. This includes queries here like term queries and other queries like phrase queries that ask a specific question about a field. Other queries, such as boolean query, combine these blocks together to arrive at an overall relevance score. Once they've combined smaller lego blocks, they in turn can be combined by other boolean queries to create even more complex query scoring and matching. \n",
      "\n",
      "When we look at the explain here, the nitty-gritty of relevance is truly highligted. We see that relevance work with a search engine is two fold:\n",
      "\n",
      "- Do I have the right lego blocks? The atomic terms stored in the index and the terms extracted from text going into the index is the fundamental building block to relevancy scoring. Do they reflect the correct features that give us the most signal as to whether a document is relevant (based on our definition of relevance)\n",
      "- Am I combining the lego blocks correctly? With each term in the index, and terms extracted from the query combining into specific individual queries, am I combining them in a way such that the resulting relevancy score reflects an accurate balancing of these indivdual factors?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Combining Each Match \n",
      "\n",
      "As described above, there are many queries that combine other queries to arrive at the score. We know from exploring the query above that we're executing a three clause boolean query. If we omit the parts that explain the individual match in the explain for \"Lego Movie\" above, we can see this boolean query in action:\n",
      "\n",
      "<pre>\n",
      "> product of:\n",
      ">   sum of:\n",
      ">      weight(_all:movie ... )\n",
      ">   coord (1/3):\n",
      "</pre>\n",
      "\n",
      "Once we've omitted the details within each match, the explain becomes simpler. Here what we have is the result of an OR query that matches only one of the three clauses. Only one SHOULD clause is in this document, in this case the term query for `movie` in the `_all` field. This match's score is then in turn summed with other scores of SHOULD clauses that match. \n",
      "\n",
      "Outside of the inner sum, the boolean query attempts to account for not all query terms matching. It applies a coordinating factor `coord` that punishes clauses where not all query terms match. In this case, only 1/3 clauses match. So the coord reduces the overall score by multiplying by 1/3. Coord provides a very powerful bias towards results where multiple clauses match. We already have this bias by taking the sum of all the matching clauses -- thus arriving at a higher score when more clauses are present. However, what coord provides is an even stronger punishment.\n",
      "\n",
      "This is simply one nesting query that we'll encounter. Other queries that we'll see throughout this book in this family incnlude\n",
      "\n",
      "- DisjunctionMaximum (known dismax)\n",
      "- DisjunctionSummation (known dismax)\n",
      "\n",
      "Each is a query strategy of combining multiple sub queries that each has its own use. We'll see much of the power of Elasticsearch's Query DSL is in creating these nested queries, allowing us to specify amazing richness in our queries.\n",
      "\n",
      "### Scoring Each Match\n",
      "\n",
      "As we mentioned, matches for `_all:movie` surprisingly get a higher score than `_all:space` which is seemingly a more specific kind of movie. Let's dig into the explain for these matches and try to understand. If we look at our match for `_all:movie` our explain adds additional layers. We see here an explanation of an individual term query matching the `_all` movie field. It roughly takes the structure:\n",
      "\n",
      "    weight (_all:movie)\n",
      "      score(_all:movie), product of:\n",
      "         queryWeight, product of:\n",
      "           idf\n",
      "           queryNorm\n",
      "         fieldWeight, product of:\n",
      "           tf\n",
      "           idf\n",
      "           fieldNorm\n",
      "           \n",
      "The `fieldWeight` calculation reflects the most important part of the match scoring -- the TF IDF calculation. Lucene uses **TF IDF** to score individual matches by default. What is TF IDF? This refers to two important pieces of term metadata extracted from the inverted index to do scoring. **TF** (tf in the scoring above) stands for \"term frequency\" and reflects how frequent a term occurs in a field. If a term occurs frequently in a field (if it mentions \"space\" a lot), we consider it much more likely to be about that term. The term is a statistical feature of a term in a field that search engines encode exactly to do ranking.\n",
      "\n",
      "Conversely, IDF tells us 1/DF. We discussed DF in chapter 2 -- the document frequency of a term. In other words, how many documents does the term occur in? If it is very popular term (like \"the\"), it will have a very high document frequency and thus matches for that term will be considered less valuable in the calculation. Rare terms on the other hand are considered very valuable.\n",
      "\n",
      "The instinct with TFIDF (or put another way TF/DF) is that term matches are scored proportion to how much of that term is concentrated in the document being scored. If the term occurs very frequency in one document, but nowhere else, it should receive a higher score. If it occurs frequently in that document, but also frequnetly elsewhere its considered less special of a term, and thus is not scored as highly.\n",
      "\n",
      "However, TF IDF by itself is often not sufficient. Should a single term match in a 1000 page book have the same impact as a single term match in a short three-sentence snippet? It can be argued that the shorter document with exactly one match is much more likely to be about that term (and thus more relevant) than the book that uses the term spuriously. For this reason, in the `fieldWeight` calculation above, TF IDF is further multiplied by fieldNorm -- a normalization factor based on the length of th edocument.\n",
      "\n",
      "Finally there's the `queryNorm`. Query Norm injects some intelligence about the relative weight of other matches into the mix. Basically it asks the question -- how valuable is this query compared to other queries we've asked about? Is it searching for a diamond in the rough (something with a very low DF). Or is it something more commonplace? Thus queryNorm attempts to regulate the matches accordingly.\n",
      "\n",
      "### _all:movie vs _all:space\n",
      "\n",
      "Finally, we can reflect on the _all:movie vs _all:space match.\n",
      "\n",
      " \n",
      "\n",
      "    \"description\": \"weight(_all:movie in 27) [PerFieldSimilarity], result of:\", \n",
      "     \"value\": 0.27179316, \n",
      "     \"details\": [\n",
      "      {\n",
      "       \"description\": \"score(doc=27,freq=1.0 = termFreq=1.0\\n), product of:\", \n",
      "       \"value\": 0.27179316, \n",
      "       \"details\": [\n",
      "        {\n",
      "         \"description\": \"queryWeight, product of:\", \n",
      "         \"value\": 0.5441669, \n",
      "         \"details\": [\n",
      "          {\n",
      "           \"description\": \"idf(docFreq=1, maxDocs=40)\", \n",
      "           \"value\": 3.9957323\n",
      "          }, \n",
      "          {\n",
      "           \"description\": \"queryNorm\", \n",
      "           \"value\": 0.13618703\n",
      "          }\n",
      "         ]\n",
      "        }, \n",
      "        {\n",
      "         \"description\": \"fieldWeight in 27, product of:\", \n",
      "         \"value\": 0.49946654, \n",
      "         \"details\": [\n",
      "          {\n",
      "           \"description\": \"tf(freq=1.0), with freq of:\", \n",
      "           \"value\": 1.0, \n",
      "           \"details\": [\n",
      "            {\n",
      "             \"description\": \"termFreq=1.0\", \n",
      "             \"value\": 1.0\n",
      "            }\n",
      "           ]\n",
      "          }, \n",
      "          {\n",
      "           \"description\": \"idf(docFreq=1, maxDocs=40)\", \n",
      "           \"value\": 3.9957323\n",
      "          }, \n",
      "          {\n",
      "           \"description\": \"fieldNorm(doc=27)\", \n",
      "           \"value\": 0.125\n",
      "          }\n",
      "         ]\n",
      "        }"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}