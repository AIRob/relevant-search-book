{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Some utilities for flattening the explain into something a bit more\n",
    "# readable. Pass Explain JSON, get something readable (ironically this is what Solr's default output is :-p)\n",
    "def flatten(l):\n",
    "    [item for sublist in l for item in sublist]\n",
    "\n",
    "def simplerExplain(explainJson, depth=0):\n",
    "    result = \" \" * (depth * 2) + \"%s, %s\\n\" % (explainJson['value'], explainJson['description'])\n",
    "    #print json.dumps(explainJson, indent=True)\n",
    "    if 'details' in explainJson:\n",
    "        for detail in explainJson['details']:\n",
    "            result += simplerExplain(detail, depth=depth+1)\n",
    "    return result\n",
    "\n",
    "# Create a stub keywords.txt at /tmp\n",
    "keywordsPath = \"/tmp/keywords.txt\";\n",
    "import os\n",
    "if os.name == \"nt\":\n",
    "    keywordsPath = \"C:\\\\temp\\keywords.txt\"\n",
    "\n",
    "f = open(\"/tmp/keywords.txt\", \"w\")\n",
    "f.write(\"maine\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"standard_clone\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"standard\",\n",
    "            \"lowercase\",\n",
    "            \"stop \"]}}}}}\n",
    "\n",
    "requests.delete(\"http://localhost:9200/my_library\")\n",
    "requests.put(\"http://localhost:9200/my_library\", data=json.dumps(settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[dr],[strangelove],[how],[i],[learned],[stop],[worrying],[love],[bomb]'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=standard_clone\", \n",
    "                    data=\"Dr. Strangelove: Or How I Learned to Stop Worrying and Love the Bomb\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"[mr],[weirdlove],[don't],[worry],[i'm],[learning],[start],[loving],[bombs]\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=standard_clone\", \n",
    "                    data=\"mr. weirdlove: don't worry, I'm learning to start loving bombs\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_english_\"},\n",
    "        \"english_keywords\": {\n",
    "          \"type\":       \"keyword_marker\",\n",
    "          \"keywords_path\":   keywordsPath},\n",
    "        \"english_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"english\"},\n",
    "        \"english_possessive_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"possessive_english\"}},\n",
    "      \"analyzer\": {\n",
    "        \"english_clone\": {\n",
    "          \"tokenizer\":  \"standard\",\n",
    "          \"filter\": [\n",
    "            \"standard\",  \n",
    "            \"english_possessive_stemmer\",\n",
    "            \"lowercase\",\n",
    "            \"english_stop\",\n",
    "            \"english_keywords\",\n",
    "            \"english_stemmer\"]}}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{\"acknowledged\":true}'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.delete(\"http://localhost:9200/my_library\")\n",
    "requests.put(\"http://localhost:9200/my_library\", data=json.dumps(settings)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[flower],[flower],[flower],[flower]'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=english_clone\", \n",
    "                    data=\"flowers flower flowered flower\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[silli],[silli],[silli],[silli]'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=english_clone\", \n",
    "                    data=\"silly silliness sillied sillying\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[maine],[main]'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=english_clone\", \n",
    "                    data=\"maine main\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[dr],[strangelov],[how],[i],[learn],[stop],[worri],[love],[bomb]'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=english_clone\", \n",
    "                    data=\"Dr. Strangelove: Or How I Learned to Stop Worrying and Love the Bomb\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"[mr],[weirdlov],[don't],[worri],[i'm],[learn],[start],[love],[bomb]\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=english_clone\", \n",
    "                    data=\"mr. weirdlove: don't worry, I'm learning to start loving bombs\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requires phonetic analysis plugin \n",
    "# https://www.elastic.co/guide/en/elasticsearch/plugins/2.1/analysis-phonetic.html\n",
    "#\n",
    "# To install on *nix systems\n",
    "# 1. Navigate to Elasticsearc directory\n",
    "# cd /usr/share/elasticsearch\n",
    "# 2. sudo bin/plugin install analysis-phonetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "settings = { \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"phonetic\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"standard\",\n",
    "            \"lowercase\",\n",
    "            \"my_doublemetaphone\"]}},\n",
    "      \"filter\": {\n",
    "        \"my_doublemetaphone\": {\n",
    "          \"type\": \"phonetic\",\n",
    "          \"encoder\": \"doublemetaphone\", \n",
    "          \"replace\": True}}}}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{\"acknowledged\":true}'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.delete(\"http://localhost:9200/my_library\")\n",
    "requests.put(\"http://localhost:9200/my_library\", data=json.dumps(settings)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[MSJ],[MSK],[FRM],[TL],[LM]'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=phonetic\", \n",
    "                    data=\"message from Dalai Lama\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'[MSJ],[MSK],[FRM],[TL],[LM]'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/_analyze?analyzer=phonetic\", \n",
    "                    data=\"massage from tall llama\")\n",
    "resp = json.loads(resp.text)\n",
    "\",\".join(['[' + token['token'] + ']'  for token in resp['tokens']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "requests.delete(\"http://localhost:9200/my_library\")\n",
    "\n",
    "settings = {\"settings\": \n",
    "    {\"number_of_shards\": 1}}\n",
    "\n",
    "requests.put(\"http://localhost:9200/my_library\", data=json.dumps(settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:9200/my_library/example/1\n",
      "{'title': 'apple apple apple apple apple'}\n",
      "http://localhost:9200/my_library/example/2\n",
      "{'title': 'apple apple apple banana banana'}\n",
      "http://localhost:9200/my_library/example/3\n",
      "{'title': 'apple banana blueberry coconut'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "documents = [\n",
    "{ \"title\":\"apple apple apple apple apple\"},\n",
    "{ \"title\":\"apple apple apple banana banana\"},\n",
    "{ \"title\":\"apple banana blueberry coconut\"},\n",
    "]\n",
    "\n",
    "for idx, document in enumerate(documents):\n",
    "    URL = \"http://localhost:9200/my_library/example/%s\" % (idx + 1)\n",
    "    print URL\n",
    "    print document\n",
    "    resp = requests.put(URL, data=json.dumps(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  {u'title': u'apple apple apple apple apple'}\n",
      "0.6968462, sum of:\n",
      "  0.6968462, weight(title:apple in 0) [PerFieldSimilarity], result of:\n",
      "    0.6968462, fieldWeight in 0, product of:\n",
      "      2.236068, tf(freq=5.0), with freq of:\n",
      "        5.0, termFreq=5.0\n",
      "      0.71231794, idf(docFreq=3, maxDocs=3)\n",
      "      0.4375, fieldNorm(doc=0)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.4038675, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.4038675, queryNorm\n",
      "\n",
      "2.  {u'title': u'apple apple apple banana banana'}\n",
      "0.5397748, sum of:\n",
      "  0.5397748, weight(title:apple in 1) [PerFieldSimilarity], result of:\n",
      "    0.5397748, fieldWeight in 1, product of:\n",
      "      1.7320508, tf(freq=3.0), with freq of:\n",
      "        3.0, termFreq=3.0\n",
      "      0.71231794, idf(docFreq=3, maxDocs=3)\n",
      "      0.4375, fieldNorm(doc=1)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.4038675, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.4038675, queryNorm\n",
      "\n",
      "3.  {u'title': u'apple banana blueberry coconut'}\n",
      "0.35615897, sum of:\n",
      "  0.35615897, weight(title:apple in 2) [PerFieldSimilarity], result of:\n",
      "    0.35615897, fieldWeight in 2, product of:\n",
      "      1.0, tf(freq=1.0), with freq of:\n",
      "        1.0, termFreq=1.0\n",
      "      0.71231794, idf(docFreq=3, maxDocs=3)\n",
      "      0.5, fieldNorm(doc=2)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.4038675, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.4038675, queryNorm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search = { \"explain\": \"true\",\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": \"apple\"}}}\n",
    "\n",
    "resp = requests.get(\"http://localhost:9200/my_library/example/_search?pretty=true\", data=json.dumps(search))\n",
    "respJson = json.loads(resp.text)\n",
    "for idx, hit in enumerate(respJson['hits']['hits']):\n",
    "    print (\"%s.  \" % (idx + 1)) + repr(hit['_source'])\n",
    "    print simplerExplain(hit['_explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extraDoc = { \"title\":\"apples apple\"}\n",
    "resp = requests.put(\"http://localhost:9200/my_library/example/%s\" % (5), data=json.dumps(extraDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  {u'title': u'apple apple apple apple apple'}\n",
      "0.6968462, sum of:\n",
      "  0.6968462, weight(title:apple in 0) [PerFieldSimilarity], result of:\n",
      "    0.6968462, fieldWeight in 0, product of:\n",
      "      2.236068, tf(freq=5.0), with freq of:\n",
      "        5.0, termFreq=5.0\n",
      "      0.71231794, idf(docFreq=3, maxDocs=3)\n",
      "      0.4375, fieldNorm(doc=0)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.4038675, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.4038675, queryNorm\n",
      "\n",
      "2.  {u'title': u'apple apple apple banana banana'}\n",
      "0.5397748, sum of:\n",
      "  0.5397748, weight(title:apple in 1) [PerFieldSimilarity], result of:\n",
      "    0.5397748, fieldWeight in 1, product of:\n",
      "      1.7320508, tf(freq=3.0), with freq of:\n",
      "        3.0, termFreq=3.0\n",
      "      0.71231794, idf(docFreq=3, maxDocs=3)\n",
      "      0.4375, fieldNorm(doc=1)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.4038675, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.4038675, queryNorm\n",
      "\n",
      "3.  {u'title': u'apple banana blueberry coconut'}\n",
      "0.35615897, sum of:\n",
      "  0.35615897, weight(title:apple in 2) [PerFieldSimilarity], result of:\n",
      "    0.35615897, fieldWeight in 2, product of:\n",
      "      1.0, tf(freq=1.0), with freq of:\n",
      "        1.0, termFreq=1.0\n",
      "      0.71231794, idf(docFreq=3, maxDocs=3)\n",
      "      0.5, fieldNorm(doc=2)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.4038675, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.4038675, queryNorm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/my_library/example/_search?pretty=true\", data=json.dumps(search))\n",
    "respJson = json.loads(resp.text)\n",
    "for idx, hit in enumerate(respJson['hits']['hits']):\n",
    "    print (\"%s.  \" % (idx + 1)) + repr(hit['_source'])\n",
    "    print simplerExplain(hit['_explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true}\n"
     ]
    }
   ],
   "source": [
    "requests.delete(\"http://localhost:9200/my_library\")\n",
    "\n",
    "settings = {\"settings\": \n",
    "    {\"number_of_shards\": 1, \n",
    "     \"index\": \n",
    "         {\"analysis\": {\n",
    "            \"analyzer\": {\n",
    "              \"default\": {\n",
    "                \"type\": \"english\"\n",
    "             \n",
    "    }}}}}\n",
    "}\n",
    "\n",
    "resp = requests.put(\"http://localhost:9200/my_library\", data=json.dumps(settings))\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:9200/my_library/example/1\n",
      "{'title': 'apple apple apple apple apple'}\n",
      "http://localhost:9200/my_library/example/2\n",
      "{'title': 'apple apple apple banana banana'}\n",
      "http://localhost:9200/my_library/example/3\n",
      "{'title': 'apple banana blueberry coconut'}\n",
      "http://localhost:9200/my_library/example/4\n",
      "{'title': 'apples apple'}\n"
     ]
    }
   ],
   "source": [
    "documents.append(extraDoc)\n",
    "\n",
    "for idx, document in enumerate(documents):\n",
    "    URL = \"http://localhost:9200/my_library/example/%s\" % (idx + 1)\n",
    "    print URL\n",
    "    print document\n",
    "    resp = requests.put(URL, data=json.dumps(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'title': u'apple apple apple apple apple'}\n",
      "0.75998294, sum of:\n",
      "  0.75998294, weight(title:appl in 0) [PerFieldSimilarity], result of:\n",
      "    0.75998294, fieldWeight in 0, product of:\n",
      "      2.236068, tf(freq=5.0), with freq of:\n",
      "        5.0, termFreq=5.0\n",
      "      0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "      0.4375, fieldNorm(doc=0)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.2872392, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.2872392, queryNorm\n",
      "\n",
      "{u'title': u'apples apple'}\n",
      "0.6866506, sum of:\n",
      "  0.6866506, weight(title:appl in 3) [PerFieldSimilarity], result of:\n",
      "    0.6866506, fieldWeight in 3, product of:\n",
      "      1.4142135, tf(freq=2.0), with freq of:\n",
      "        2.0, termFreq=2.0\n",
      "      0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "      0.625, fieldNorm(doc=3)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.2872392, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.2872392, queryNorm\n",
      "\n",
      "{u'title': u'apple apple apple banana banana'}\n",
      "0.58868027, sum of:\n",
      "  0.58868027, weight(title:appl in 1) [PerFieldSimilarity], result of:\n",
      "    0.58868027, fieldWeight in 1, product of:\n",
      "      1.7320508, tf(freq=3.0), with freq of:\n",
      "        3.0, termFreq=3.0\n",
      "      0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "      0.4375, fieldNorm(doc=1)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.2872392, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.2872392, queryNorm\n",
      "\n",
      "{u'title': u'apple banana blueberry coconut'}\n",
      "0.3884282, sum of:\n",
      "  0.3884282, weight(title:appl in 2) [PerFieldSimilarity], result of:\n",
      "    0.3884282, fieldWeight in 2, product of:\n",
      "      1.0, tf(freq=1.0), with freq of:\n",
      "        1.0, termFreq=1.0\n",
      "      0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "      0.5, fieldNorm(doc=2)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    1.2872392, _type:example, product of:\n",
      "      1.0, boost\n",
      "      1.2872392, queryNorm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search = { \"explain\": \"true\",\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": \"apple\"}}}\n",
    "\n",
    "resp = requests.get(\"http://localhost:9200/my_library/example/_search?pretty=true\", data=json.dumps(search))\n",
    "respJson = json.loads(resp.text)\n",
    "for hit in respJson['hits']['hits']:\n",
    "    print hit['_source']\n",
    "    print simplerExplain(hit['_explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  {u'title': u'apple apple apple banana banana'}\n",
      "0.9862758, sum of:\n",
      "  0.9862758, sum of:\n",
      "    0.30409503, weight(title:appl in 1) [PerFieldSimilarity], result of:\n",
      "      0.30409503, score(doc=1,freq=3.0), product of:\n",
      "        0.5165708, queryWeight, product of:\n",
      "          0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "          0.6649502, queryNorm\n",
      "        0.58868027, fieldWeight in 1, product of:\n",
      "          1.7320508, tf(freq=3.0), with freq of:\n",
      "            3.0, termFreq=3.0\n",
      "          0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "          0.4375, fieldNorm(doc=1)\n",
      "    0.68218076, weight(title:banana in 1) [PerFieldSimilarity], result of:\n",
      "      0.68218076, score(doc=1,freq=2.0), product of:\n",
      "        0.85624444, queryWeight, product of:\n",
      "          1.287682, idf(docFreq=2, maxDocs=4)\n",
      "          0.6649502, queryNorm\n",
      "        0.79671264, fieldWeight in 1, product of:\n",
      "          1.4142135, tf(freq=2.0), with freq of:\n",
      "            2.0, termFreq=2.0\n",
      "          1.287682, idf(docFreq=2, maxDocs=4)\n",
      "          0.4375, fieldNorm(doc=1)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    0.6649502, _type:example, product of:\n",
      "      1.0, boost\n",
      "      0.6649502, queryNorm\n",
      "\n",
      "2.  {u'title': u'apple banana blueberry coconut'}\n",
      "0.751936, sum of:\n",
      "  0.751936, sum of:\n",
      "    0.20065068, weight(title:appl in 2) [PerFieldSimilarity], result of:\n",
      "      0.20065068, score(doc=2,freq=1.0), product of:\n",
      "        0.5165708, queryWeight, product of:\n",
      "          0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "          0.6649502, queryNorm\n",
      "        0.3884282, fieldWeight in 2, product of:\n",
      "          1.0, tf(freq=1.0), with freq of:\n",
      "            1.0, termFreq=1.0\n",
      "          0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "          0.5, fieldNorm(doc=2)\n",
      "    0.5512853, weight(title:banana in 2) [PerFieldSimilarity], result of:\n",
      "      0.5512853, score(doc=2,freq=1.0), product of:\n",
      "        0.85624444, queryWeight, product of:\n",
      "          1.287682, idf(docFreq=2, maxDocs=4)\n",
      "          0.6649502, queryNorm\n",
      "        0.643841, fieldWeight in 2, product of:\n",
      "          1.0, tf(freq=1.0), with freq of:\n",
      "            1.0, termFreq=1.0\n",
      "          1.287682, idf(docFreq=2, maxDocs=4)\n",
      "          0.5, fieldNorm(doc=2)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    0.6649502, _type:example, product of:\n",
      "      1.0, boost\n",
      "      0.6649502, queryNorm\n",
      "\n",
      "3.  {u'title': u'apple apple apple apple apple'}\n",
      "0.1962925, sum of:\n",
      "  0.1962925, product of:\n",
      "    0.392585, sum of:\n",
      "      0.392585, weight(title:appl in 0) [PerFieldSimilarity], result of:\n",
      "        0.392585, score(doc=0,freq=5.0), product of:\n",
      "          0.5165708, queryWeight, product of:\n",
      "            0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "            0.6649502, queryNorm\n",
      "          0.75998294, fieldWeight in 0, product of:\n",
      "            2.236068, tf(freq=5.0), with freq of:\n",
      "              5.0, termFreq=5.0\n",
      "            0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "            0.4375, fieldNorm(doc=0)\n",
      "    0.5, coord(1/2)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    0.6649502, _type:example, product of:\n",
      "      1.0, boost\n",
      "      0.6649502, queryNorm\n",
      "\n",
      "4.  {u'title': u'apples apple'}\n",
      "0.17735182, sum of:\n",
      "  0.17735182, product of:\n",
      "    0.35470363, sum of:\n",
      "      0.35470363, weight(title:appl in 3) [PerFieldSimilarity], result of:\n",
      "        0.35470363, score(doc=3,freq=2.0), product of:\n",
      "          0.5165708, queryWeight, product of:\n",
      "            0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "            0.6649502, queryNorm\n",
      "          0.6866506, fieldWeight in 3, product of:\n",
      "            1.4142135, tf(freq=2.0), with freq of:\n",
      "              2.0, termFreq=2.0\n",
      "            0.7768564, idf(docFreq=4, maxDocs=4)\n",
      "            0.625, fieldNorm(doc=3)\n",
      "    0.5, coord(1/2)\n",
      "  0.0, match on required clause, product of:\n",
      "    0.0, # clause\n",
      "    0.6649502, _type:example, product of:\n",
      "      1.0, boost\n",
      "      0.6649502, queryNorm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search = { \"explain\": \"true\",\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": \"apple banana\"}}}\n",
    "\n",
    "resp = requests.get(\"http://localhost:9200/my_library/example/_search?pretty=true\", data=json.dumps(search))\n",
    "respJson = json.loads(resp.text)\n",
    "for idx, hit in enumerate(respJson['hits']['hits']):\n",
    "    print (\"%s.  \" % (idx + 1)) + repr(hit['_source'])\n",
    "    print simplerExplain(hit['_explanation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 -- Acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"acronyms\": {\n",
    "          \"type\": \"word_delimiter\",\n",
    "          \"catenate_all\": True,\n",
    "          \"generate_word_parts\": False,\n",
    "          \"generateNumberParts\": False}},\n",
    "      \"analyzer\": {\n",
    "        \"standard_with_acronyms\": {\n",
    "          \"tokenizer\": \"whitespace\",\n",
    "          \"filter\": [\"standard\",\"lowercase\",\"acronyms\"]}}}}}\n",
    "\n",
    "requests.delete(\"http://localhost:9200/example\")\n",
    "resp = requests.post(\"http://localhost:9200/example\", data=json.dumps(data))\n",
    "print resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tokens:\n",
      "- token: \"ibm\"\n",
      "  start_offset: 0\n",
      "  end_offset: 5\n",
      "  type: \"word\"\n",
      "  position: 0\n",
      "- token: \"versus\"\n",
      "  start_offset: 7\n",
      "  end_offset: 13\n",
      "  type: \"word\"\n",
      "  position: 1\n",
      "- token: \"ibm\"\n",
      "  start_offset: 14\n",
      "  end_offset: 17\n",
      "  type: \"word\"\n",
      "  position: 2\n",
      "- token: \"versus\"\n",
      "  start_offset: 18\n",
      "  end_offset: 24\n",
      "  type: \"word\"\n",
      "  position: 3\n",
      "- token: \"ibm\"\n",
      "  start_offset: 25\n",
      "  end_offset: 28\n",
      "  type: \"word\"\n",
      "  position: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/example/_analyze?analyzer=standard_with_acronyms&format=yaml\", \n",
    "                     data=\"I.B.M. versus IBM versus ibm\")\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tokens:\n",
      "- token: \"18008675309\"\n",
      "  start_offset: 0\n",
      "  end_offset: 14\n",
      "  type: \"word\"\n",
      "  position: 0\n",
      "\n",
      "---\n",
      "tokens:\n",
      "- token: \"18008675309\"\n",
      "  start_offset: 0\n",
      "  end_offset: 14\n",
      "  type: \"word\"\n",
      "  position: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/example/_analyze?analyzer=standard_with_acronyms&format=yaml\", \n",
    "                     data=\"1.800.867.5309\")\n",
    "print resp.text\n",
    "resp = requests.get(\"http://localhost:9200/example/_analyze?analyzer=standard_with_acronyms&format=yaml\", \n",
    "                     data=\"1-800-867-5309\")\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"phone_num_filter\": {\n",
    "          \"type\": \"word_delimiter\",\n",
    "          \"catenate_all\": True,\n",
    "          \"generate_number_parts\": False},\n",
    "        \"phone_num_parts\": {\n",
    "          \"type\": \"pattern_capture\",\n",
    "          \"patterns\": [\"(\\\\d{7}$)\",\"(\\\\d{10}$)\"],\n",
    "          \"preserve_original\": True}},\n",
    "      \"analyzer\": {\n",
    "        \"phone_num\": {\n",
    "          \"tokenizer\": \"keyword\",\n",
    "          \"filter\": [\"phone_num_filter\",\"phone_num_parts\"]}}}}}\n",
    "requests.delete(\"http://localhost:9200/example\")\n",
    "resp = requests.post(\"http://localhost:9200/example\", data=json.dumps(data))\n",
    "print resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tokens:\n",
      "- token: \"18008675309\"\n",
      "  start_offset: 0\n",
      "  end_offset: 14\n",
      "  type: \"word\"\n",
      "  position: 0\n",
      "- token: \"8008675309\"\n",
      "  start_offset: 0\n",
      "  end_offset: 14\n",
      "  type: \"word\"\n",
      "  position: 0\n",
      "- token: \"8675309\"\n",
      "  start_offset: 0\n",
      "  end_offset: 14\n",
      "  type: \"word\"\n",
      "  position: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(\"http://localhost:9200/example/_analyze?analyzer=phone_num&format=yaml\", \n",
    "                     data=\"1(800)867-5309\")\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4.2 Capturing Meaning with Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"acknowledged\":true}\n",
      "200\n",
      "{\"acknowledged\":true}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_english_\"},\n",
    "        \"english_keywords\": {\n",
    "          \"type\":       \"keyword_marker\",\n",
    "          \"keywords_path\":   \"/tmp/keywords.txt\"},\n",
    "        \"english_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"english\"},\n",
    "        \"english_possessive_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"possessive_english\"},\n",
    "        \"retail_syn_filter\": {  # 1. our new filter\n",
    "          \"type\": \"synonym\",\n",
    "          \"synonyms\": [\n",
    "            \"dress shoe,dress shoes => dress_shoe, shoe\"]\n",
    "         }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"retail_analyzer\": { # 2. our new analyzer\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\":[\n",
    "            \"english_possessive_stemmer\",\n",
    "            \"lowercase\",\n",
    "            \"retail_syn_filter\",  # 2. here’s where the filter is used \n",
    "            \"english_stop\",\n",
    "            \"english_keywords\",\n",
    "            \"english_stemmer\"]}}}},\n",
    "  \"mappings\": {\n",
    "    \"items\": {\n",
    "      \"properties\": {\n",
    "        \"desc\": {  # 3. our desc field includes synonyms\n",
    "          \"type\": \"string\",\n",
    "          \"analyzer\": \"retail_analyzer\"}}}}}\n",
    "        \n",
    "requests.delete(\"http://localhost:9200/retail\")\n",
    "print resp.status_code\n",
    "print resp.text\n",
    "resp = requests.put(\"http://localhost:9200/retail\", data=json.dumps(data))\n",
    "print resp.status_code\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = { \"desc\": \"bob's brand dress shoes are the bomb diggity\"} # “dress shoe” document\n",
    "requests.post(\"http://localhost:9200/retail/items/1\",  \n",
    "             data=json.dumps(data))\n",
    "\n",
    "data = { \"desc\": \"this little black dress is sure to impress\"}\n",
    "requests.post(\"http://localhost:9200/retail/items/2\",  # “dress” document\n",
    "             data=json.dumps(data))\n",
    "\n",
    "data = { \"desc\": \"tennis shoes... you know, for tennis\"}\n",
    "requests.post(\"http://localhost:9200/retail/items/3\",  # “shoe” document\n",
    "             data=json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "took: 2\n",
      "timed_out: false\n",
      "_shards:\n",
      "  total: 5\n",
      "  successful: 5\n",
      "  failed: 0\n",
      "hits:\n",
      "  total: 1\n",
      "  max_score: 0.13424811\n",
      "  hits:\n",
      "  - _index: \"retail\"\n",
      "    _type: \"items\"\n",
      "    _id: \"2\"\n",
      "    _score: 0.13424811\n",
      "    _source:\n",
      "      desc: \"this little black dress is sure to impress\"\n",
      "\n",
      "---\n",
      "valid: true\n",
      "_shards:\n",
      "  total: 1\n",
      "  successful: 1\n",
      "  failed: 0\n",
      "explanations:\n",
      "- index: \"retail\"\n",
      "  valid: true\n",
      "  explanation: \"desc:dress\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "          \"desc\": \"dress\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp= requests.get(\"http://localhost:9200/retail/_search?format=yaml\", data=json.dumps(search))\n",
    "print resp.text                   \n",
    "resp= requests.get(\"http://localhost:9200/retail/_validate/query?explain&format=yaml\", data=json.dumps(search))\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "took: 2\n",
      "timed_out: false\n",
      "_shards:\n",
      "  total: 5\n",
      "  successful: 5\n",
      "  failed: 0\n",
      "hits:\n",
      "  total: 2\n",
      "  max_score: 0.13424811\n",
      "  hits:\n",
      "  - _index: \"retail\"\n",
      "    _type: \"items\"\n",
      "    _id: \"1\"\n",
      "    _score: 0.13424811\n",
      "    _source:\n",
      "      desc: \"bob's brand dress shoes are the bomb diggity\"\n",
      "  - _index: \"retail\"\n",
      "    _type: \"items\"\n",
      "    _id: \"3\"\n",
      "    _score: 0.13424811\n",
      "    _source:\n",
      "      desc: \"tennis shoes... you know, for tennis\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "          \"desc\": \"shoe\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp= requests.get(\"http://localhost:9200/retail/_search?format=yaml\", data=json.dumps(search))\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tokens:\n",
      "- token: \"bob\"\n",
      "  start_offset: 0\n",
      "  end_offset: 5\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 0\n",
      "- token: \"brand\"\n",
      "  start_offset: 6\n",
      "  end_offset: 11\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 1\n",
      "- token: \"dress_sho\"\n",
      "  start_offset: 12\n",
      "  end_offset: 23\n",
      "  type: \"SYNONYM\"\n",
      "  position: 2\n",
      "- token: \"shoe\"\n",
      "  start_offset: 12\n",
      "  end_offset: 23\n",
      "  type: \"SYNONYM\"\n",
      "  position: 2\n",
      "- token: \"bomb\"\n",
      "  start_offset: 32\n",
      "  end_offset: 36\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 5\n",
      "- token: \"diggiti\"\n",
      "  start_offset: 37\n",
      "  end_offset: 44\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp= requests.get(\"http://localhost:9200/retail/_analyze?analyzer=retail_analyzer&format=yaml\", \n",
    "                   data=\"bob's brand dress shoes are the bomb diggity\")\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tokens:\n",
      "- token: \"tenni\"\n",
      "  start_offset: 0\n",
      "  end_offset: 6\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 0\n",
      "- token: \"shoe\"\n",
      "  start_offset: 7\n",
      "  end_offset: 12\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 1\n",
      "- token: \"you\"\n",
      "  start_offset: 16\n",
      "  end_offset: 19\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 2\n",
      "- token: \"know\"\n",
      "  start_offset: 20\n",
      "  end_offset: 24\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 3\n",
      "- token: \"tenni\"\n",
      "  start_offset: 30\n",
      "  end_offset: 36\n",
      "  type: \"<ALPHANUM>\"\n",
      "  position: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp= requests.get(\"http://localhost:9200/retail/_analyze?analyzer=retail_analyzer&format=yaml\", \n",
    "                   data=\"tennis shoes... you know, for tennis\")\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"acknowledged\":true}\n",
      "200\n",
      "{\"acknowledged\":true}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_english_\"},\n",
    "        \"english_keywords\": {\n",
    "          \"type\":       \"keyword_marker\",\n",
    "          \"keywords_path\":   \"/tmp/keywords.txt\"},\n",
    "        \"english_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"english\"},\n",
    "        \"english_possessive_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"possessive_english\"},\n",
    "         \"retail_syn_filter_index\": {\n",
    "          \"type\": \"synonym\",\n",
    "          \"synonyms\": [\"dress shoe,dress shoes => dress_shoe, shoe\"]\n",
    "         },\n",
    "         \"retail_syn_filter_search\": {\n",
    "           \"type\": \"synonym\",\n",
    "           \"synonyms\": [\"dress shoe,dress shoes => dress_shoe\"]\n",
    "          }                 \n",
    "      },\n",
    "      \"analyzer\": {\n",
    "         \"retail_analyzer_index\": {\n",
    "            \"tokenizer\": \"standard\",\n",
    "            \"filter\": [\n",
    "              \"english_possessive_stemmer\",\n",
    "              \"lowercase\",\n",
    "              \"retail_syn_filter_index\",\n",
    "              \"english_stop\",\n",
    "              \"english_keywords\",\n",
    "              \"english_stemmer\"\n",
    "            ]},\n",
    "          \"retail_analyzer_search\": {\n",
    "             \"tokenizer\": \"standard\",\n",
    "             \"filter\": [\n",
    "               \"english_possessive_stemmer\",\n",
    "               \"lowercase\",\n",
    "               \"retail_syn_filter_search\",\n",
    "                \"english_stop\",\n",
    "                \"english_keywords\",\n",
    "                \"english_stemmer\"\n",
    "            ]}            \n",
    "            }}},\n",
    "  \"mappings\": {\n",
    "    \"items\": {\n",
    "      \"properties\": {\n",
    "        \"desc\": {  # 3. our desc field includes synonyms\n",
    "          \"type\": \"string\",\n",
    "          \"analyzer\": \"retail_analyzer_index\",\n",
    "          \"search_analyzer\": \"retail_analyzer_search\"}}}}}\n",
    "        \n",
    "requests.delete(\"http://localhost:9200/retail\")\n",
    "print resp.status_code\n",
    "print resp.text\n",
    "resp = requests.put(\"http://localhost:9200/retail\", data=json.dumps(data))\n",
    "print resp.status_code\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4.3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"acknowledged\":true}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = { \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"path_hierarchy\": {\n",
    "          \"tokenizer\": \"path_hierarchy\"}}}},\n",
    "  \"mappings\": {\n",
    "      \"item\": {\n",
    "        \"properties\": {\n",
    "          \"inventory_dir\": {\n",
    "            \"type\": \"string\",\n",
    "            \"analyzer\": \"path_hierarchy\"}}}}}\n",
    "\n",
    "requests.delete(\"http://localhost:9200/catalog\")\n",
    "resp = requests.put(\"http://localhost:9200/catalog\", data=json.dumps(settings))\n",
    "print resp.status_code\n",
    "print resp.text\n",
    "\n",
    "data = { \"inventory_dir\":\"/fruit/apples/fuji\",\n",
    "          \"description\":\"crisp, sweet-flavoured, long shelf-life\"}\n",
    "requests.put(\"http://localhost:9200/catalog/item/1\", data=json.dumps(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "took: 1\n",
      "timed_out: false\n",
      "_shards:\n",
      "  total: 5\n",
      "  successful: 5\n",
      "  failed: 0\n",
      "hits:\n",
      "  total: 1\n",
      "  max_score: 0.11506981\n",
      "  hits:\n",
      "  - _index: \"catalog\"\n",
      "    _type: \"item\"\n",
      "    _id: \"1\"\n",
      "    _score: 0.11506981\n",
      "    _source:\n",
      "      inventory_dir: \"/fruit/apples/fuji\"\n",
      "      description: \"crisp, sweet-flavoured, long shelf-life\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \\\n",
    "{ \"query\": {\n",
    "    \"bool\": {\n",
    "      \"should\":\n",
    "        [{\"match\": {\"description\": \"shelf\"}}],\n",
    "      \"filter\": [\n",
    "         {\"term\": {\"inventory_dir\": \"/fruit/apples/fuji\"}}]}}}\n",
    "\n",
    "\n",
    "resp = requests.get(\"http://localhost:9200/catalog/_search?format=yaml\", data=json.dumps(query))\n",
    "print resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
